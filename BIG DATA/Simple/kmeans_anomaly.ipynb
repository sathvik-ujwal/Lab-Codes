{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sathvik/.local/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pyspark\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, filter\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "import requests\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/03 23:32:11 WARN Utils: Your hostname, sathvik-HP-EliteBook-x360-1030-G2 resolves to a loopback address: 127.0.1.1; using 192.168.0.108 instead (on interface wlp58s0)\n",
      "24/11/03 23:32:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/03 23:32:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/11/03 23:32:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Anomaly detection using KMeans\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "local_path = \"../Data/iris.csv\"\n",
    "\n",
    "response = requests.get(url)\n",
    "with open(local_path, 'wb') as f:\n",
    "  f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|      class|\n",
      "+------------+-----------+------------+-----------+-----------+\n",
      "|         5.1|        3.5|         1.4|        0.2|Iris-setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2|Iris-setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2|Iris-setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2|Iris-setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2|Iris-setosa|\n",
      "+------------+-----------+------------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(local_path, header=False, inferSchema=True)\n",
    "columns = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"]\n",
    "\n",
    "df = df.toDF(*columns)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal_length: double (nullable = true)\n",
      " |-- sepal_width: double (nullable = true)\n",
      " |-- petal_length: double (nullable = true)\n",
      " |-- petal_width: double (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/03 23:33:37 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+------------------+------------------+--------------+\n",
      "|summary|      sepal_length|        sepal_width|      petal_length|       petal_width|         class|\n",
      "+-------+------------------+-------------------+------------------+------------------+--------------+\n",
      "|  count|               150|                150|               150|               150|           150|\n",
      "|   mean| 5.843333333333335| 3.0540000000000007|3.7586666666666693|1.1986666666666672|          NULL|\n",
      "| stddev|0.8280661279778637|0.43359431136217375| 1.764420419952262|0.7631607417008414|          NULL|\n",
      "|    min|               4.3|                2.0|               1.0|               0.1|   Iris-setosa|\n",
      "|    max|               7.9|                4.4|               6.9|               2.5|Iris-virginica|\n",
      "+-------+------------------+-------------------+------------------+------------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"],\n",
    "                            outputCol=\"features\")\n",
    "\n",
    "assembled_df = assembler.transform(df)\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "scaler_model = scaler.fit(assembled_df)\n",
    "scaled_df = scaler_model.transform(assembled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-----------+-----------------+--------------------+--------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|      class|         features|     scaled_features|clusters|\n",
      "+------------+-----------+------------+-----------+-----------+-----------------+--------------------+--------+\n",
      "|         5.1|        3.5|         1.4|        0.2|Iris-setosa|[5.1,3.5,1.4,0.2]|[6.15892840883878...|       1|\n",
      "|         4.9|        3.0|         1.4|        0.2|Iris-setosa|[4.9,3.0,1.4,0.2]|[5.9174018045706,...|       1|\n",
      "|         4.7|        3.2|         1.3|        0.2|Iris-setosa|[4.7,3.2,1.3,0.2]|[5.67587520030241...|       1|\n",
      "|         4.6|        3.1|         1.5|        0.2|Iris-setosa|[4.6,3.1,1.5,0.2]|[5.55511189816831...|       1|\n",
      "|         5.0|        3.6|         1.4|        0.2|Iris-setosa|[5.0,3.6,1.4,0.2]|[6.03816510670469...|       1|\n",
      "+------------+-----------+------------+-----------+-----------+-----------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(k=3, seed=1, featuresCol=\"scaled_features\", predictionCol=\"clusters\")\n",
    "model = kmeans.fit(scaled_df)\n",
    "clustered_df = model.transform(scaled_df)\n",
    "\n",
    "clustered_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1 Centers: [8.08674985 7.05728816 3.07079977 2.53967705]\n",
      "Cluster 2 Centers: [6.05788156 7.93556612 0.83163112 0.31822541]\n",
      "Cluster 3 Centers: [6.8887588  6.07660788 2.38900964 1.7461704 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "centers = model.clusterCenters()\n",
    "for i , center in enumerate(centers):\n",
    "  print(f\"Cluster {i+1} Centers: {center}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(features, center):\n",
    "    return float(np.sqrt(np.sum((np.array(features) - np.array(center)) ** 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal_length: double (nullable = true)\n",
      " |-- sepal_width: double (nullable = true)\n",
      " |-- petal_length: double (nullable = true)\n",
      " |-- petal_width: double (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- scaled_features: vector (nullable = true)\n",
      " |-- clusters: integer (nullable = false)\n",
      " |-- distance: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "distance_udf = udf(lambda point, cluster: euclidean_distance(point, centers[cluster]), DoubleType())\n",
    "\n",
    "distance_df = clustered_df.withColumn(\"distance\", distance_udf(col(\"scaled_features\"), col(\"clusters\")))\n",
    "\n",
    "threshold = 1.0\n",
    "anomalies = distance_df.filter(col(\"distance\") > threshold)\n",
    "anomalies.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sathvik/.local/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of anomalies: 46\n",
      "Number of normal data points: 104\n",
      "Total data points: 150\n"
     ]
    }
   ],
   "source": [
    "distance_df = distance_df.withColumn(\"is_anomaly\", col(\"distance\") > threshold)\n",
    "num_anomalies = distance_df.filter(col(\"is_anomaly\")).count()\n",
    "num_normal = distance_df.filter(~col(\"is_anomaly\")).count()\n",
    "total_count = distance_df.count()\n",
    "\n",
    "print(f\"Total number of anomalies: {num_anomalies}\")\n",
    "print(f\"Number of normal data points: {num_normal}\")\n",
    "print(f\"Total data points: {total_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(local_path)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
