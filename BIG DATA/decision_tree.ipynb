{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/02 22:05:09 WARN Utils: Your hostname, sathvik-HP-EliteBook-x360-1030-G2 resolves to a loopback address: 127.0.1.1; using 192.168.0.108 instead (on interface wlp58s0)\n",
      "24/11/02 22:05:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/02 22:05:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/11/02 22:05:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/11/02 22:05:14 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: double (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education_num: double (nullable = true)\n",
      " |-- marital_status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital_gain: double (nullable = true)\n",
      " |-- capital_loss: double (nullable = true)\n",
      " |-- hours_per_week: double (nullable = true)\n",
      " |-- native_country: string (nullable = true)\n",
      " |-- income: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/02 22:05:37 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------+------------------+-------------+-----------------+--------------+-----------------+------------+-------------------+-------+------------------+----------------+------------------+--------------+------+\n",
      "|summary|               age|   workclass|            fnlwgt|    education|    education_num|marital_status|       occupation|relationship|               race|    sex|      capital_gain|    capital_loss|    hours_per_week|native_country|income|\n",
      "+-------+------------------+------------+------------------+-------------+-----------------+--------------+-----------------+------------+-------------------+-------+------------------+----------------+------------------+--------------+------+\n",
      "|  count|             32561|       32561|             32561|        32561|            32561|         32561|            32561|       32561|              32561|  32561|             32561|           32561|             32561|         32561| 32561|\n",
      "|   mean| 38.58164675532078|        NULL|189778.36651208502|         NULL| 10.0806793403151|          NULL|             NULL|        NULL|               NULL|   NULL|1077.6488437087312| 87.303829734959|40.437455852092995|          NULL|  NULL|\n",
      "| stddev|13.640432553581356|        NULL|105549.97769702227|         NULL|2.572720332067397|          NULL|             NULL|        NULL|               NULL|   NULL| 7385.292084840354|402.960218649002|12.347428681731838|          NULL|  NULL|\n",
      "|    min|                17|           ?|           12285.0|         10th|              1.0|      Divorced|                ?|     Husband| Amer-Indian-Eskimo| Female|               0.0|             0.0|               1.0|             ?| <=50K|\n",
      "|    max|                90| Without-pay|         1484705.0| Some-college|             16.0|       Widowed| Transport-moving|        Wife|              White|   Male|           99999.0|          4356.0|              99.0|    Yugoslavia|  >50K|\n",
      "+-------+------------------+------------+------------------+-------------+-----------------+--------------+-----------------+------------+-------------------+-------+------------------+----------------+------------------+--------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"IncomePrediction\").getOrCreate()\n",
    "\n",
    "# Load dataset into PySpark DataFrame\n",
    "df = spark.read.csv(\"Data/adult.csv\", header=False, inferSchema=True)\n",
    "\n",
    "# Rename columns for better understanding\n",
    "df = df.withColumnRenamed(\"_c0\", \"age\")\\\n",
    "       .withColumnRenamed(\"_c1\", \"workclass\")\\\n",
    "       .withColumnRenamed(\"_c2\", \"fnlwgt\")\\\n",
    "       .withColumnRenamed(\"_c3\", \"education\")\\\n",
    "       .withColumnRenamed(\"_c4\", \"education_num\")\\\n",
    "       .withColumnRenamed(\"_c5\", \"marital_status\")\\\n",
    "       .withColumnRenamed(\"_c6\", \"occupation\")\\\n",
    "       .withColumnRenamed(\"_c7\", \"relationship\")\\\n",
    "       .withColumnRenamed(\"_c8\", \"race\")\\\n",
    "       .withColumnRenamed(\"_c9\", \"sex\")\\\n",
    "       .withColumnRenamed(\"_c10\", \"capital_gain\")\\\n",
    "       .withColumnRenamed(\"_c11\", \"capital_loss\")\\\n",
    "       .withColumnRenamed(\"_c12\", \"hours_per_week\")\\\n",
    "       .withColumnRenamed(\"_c13\", \"native_country\")\\\n",
    "       .withColumnRenamed(\"_c14\", \"income\")\n",
    "\n",
    "# Show basic statistics and information\n",
    "df.printSchema()\n",
    "df.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sathvik/.local/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|            features|income_index|\n",
      "+--------------------+------------+\n",
      "|[39.0,77516.0,13....|         0.0|\n",
      "|(14,[0,1,2,5,6,7,...|         0.0|\n",
      "|(14,[0,1,2,5,8,9,...|         0.0|\n",
      "|(14,[0,1,2,5,7,9,...|         0.0|\n",
      "|[28.0,338409.0,13...|         0.0|\n",
      "+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Handling missing values\n",
    "# Dropping rows with missing values\n",
    "df_clean = df.dropna()\n",
    "\n",
    "# Handling categorical features using StringIndexer\n",
    "categorical_columns = ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country', 'income']\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\") for column in categorical_columns]\n",
    "\n",
    "# Assemble features into a feature vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week'] + [col+\"_index\" for col in categorical_columns[:-1]],\n",
    "    outputCol='features'\n",
    ")\n",
    "\n",
    "# Create a pipeline for preprocessing\n",
    "pipeline = Pipeline(stages=indexers + [assembler])\n",
    "df_prepared = pipeline.fit(df_clean).transform(df_clean)\n",
    "\n",
    "# Show the prepared dataset with features column\n",
    "df_prepared.select(\"features\", \"income_index\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+----------+\n",
      "|            features|income_index|prediction|\n",
      "+--------------------+------------+----------+\n",
      "|[17.0,41643.0,7.0...|         0.0|       0.0|\n",
      "|[17.0,64785.0,6.0...|         0.0|       0.0|\n",
      "|[17.0,80077.0,7.0...|         0.0|       0.0|\n",
      "|[17.0,104025.0,7....|         0.0|       0.0|\n",
      "|[17.0,139183.0,6....|         0.0|       0.0|\n",
      "+--------------------+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = df_prepared.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Create and train the Decision Tree model\n",
    "dt = DecisionTreeClassifier(labelCol=\"income_index\", featuresCol=\"features\", maxBins=45)\n",
    "dt_model = dt.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = dt_model.transform(test_data)\n",
    "\n",
    "# Show sample predictions\n",
    "predictions.select(\"features\", \"income_index\", \"prediction\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.84\n",
      "Test Precision: 0.84\n",
      "Test Recall: 0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"income_index\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "\n",
    "# Calculate precision\n",
    "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"income_index\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "precision = precision_evaluator.evaluate(predictions)\n",
    "\n",
    "# Calculate recall\n",
    "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"income_index\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "recall = recall_evaluator.evaluate(predictions)\n",
    "\n",
    "# Print accuracy, precision, and recall\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Test Precision: {precision:.2f}\")\n",
    "print(f\"Test Recall: {recall:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
