{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sathvik/.local/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "24/11/02 22:00:57 WARN Utils: Your hostname, sathvik-HP-EliteBook-x360-1030-G2 resolves to a loopback address: 127.0.1.1; using 192.168.0.108 instead (on interface wlp58s0)\n",
      "24/11/02 22:00:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/02 22:00:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/11/02 22:01:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/home/sathvik/Documents/pyspark/kddcup.data_10_percent_corrected.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataPreprocessing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkddcup.data_10_percent_corrected\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Define the schema based on the dataset\u001b[39;00m\n\u001b[1;32m     18\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mtoDF(\u001b[38;5;241m*\u001b[39m[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m43\u001b[39m)] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/home/sathvik/Documents/pyspark/kddcup.data_10_percent_corrected."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "import numpy as np\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"DataPreprocessing\").getOrCreate()\n",
    "\n",
    "# Load data\n",
    "data = spark.read.csv(\"kddcup.data_10_percent_corrected\", header=False, inferSchema=True)\n",
    "\n",
    "# Define the schema based on the dataset\n",
    "data = data.toDF(*[f\"col{i}\" for i in range(1, 43)] + [\"label\"])\n",
    "\n",
    "# Handle missing values\n",
    "feature_cols = data.columns[4:-1]  # All numerical features except the label\n",
    "\n",
    "imputer = Imputer(inputCols=feature_cols, outputCols=feature_cols).setStrategy(\"mean\")\n",
    "data = imputer.fit(data).transform(data)\n",
    "\n",
    "# Scale numerical features\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "data = assembler.transform(data)\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "data = scaler.fit(data).transform(data)\n",
    "\n",
    "data.show(3)\n",
    "\n",
    "# K-means clustering\n",
    "kmeans = KMeans(k=80, seed=1, featuresCol=\"scaled_features\", predictionCol=\"cluster\")\n",
    "model = kmeans.fit(data)\n",
    "clusters = model.transform(data)\n",
    "\n",
    "# Get cluster centers\n",
    "centers = np.array(model.clusterCenters())\n",
    "\n",
    "# Calculate distances from points to cluster centers\n",
    "def distance_to_center(features, center):\n",
    "    return float(np.sqrt(np.sum((np.array(features) - np.array(center)) ** 2)))\n",
    "\n",
    "distance_udf = udf(lambda features: min([distance_to_center(features, center) for center in centers]), FloatType())\n",
    "clusters = clusters.withColumn(\"distance_to_center\", distance_udf(col(\"scaled_features\")))\n",
    "\n",
    "# Define anomaly if distance is greater than a threshold\n",
    "threshold = 1.0  # Set a suitable threshold\n",
    "clusters = clusters.withColumn(\"is_anomaly\", col(\"distance_to_center\") > threshold)\n",
    "\n",
    "clusters.show(3)\n",
    "\n",
    "# Evaluating K-Means Clustering for anomalies\n",
    "evaluator = ClusteringEvaluator(featuresCol=\"scaled_features\", predictionCol=\"cluster\")\n",
    "silhouette = evaluator.evaluate(clusters)\n",
    "print(f\"Silhouette with squared Euclidean distance = {silhouette}\")\n",
    "\n",
    "# Print the number of anomalies and normal points\n",
    "num_anomalies = clusters.filter(col(\"is_anomaly\")).count()\n",
    "num_normal = clusters.filter(~col(\"is_anomaly\")).count()\n",
    "total_count = clusters.count()\n",
    "\n",
    "print(f\"Number of anomalies: {num_anomalies}\")\n",
    "print(f\"Number of normal points: {num_normal}\")\n",
    "print(f\"Total number of points: {total_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "import numpy as np\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"DataPreprocessing\").getOrCreate()\n",
    "\n",
    "# Load data\n",
    "data = spark.read.csv(\"kddcup.data_10_percent_corrected\", header=False, inferSchema=True)\n",
    "\n",
    "# Define the schema based on the dataset\n",
    "data = data.toDF(\"col1\", \"col2\", \"col3\", \"col4\", \"col5\", \"col6\", \"col7\", \"col8\", \"col9\", \"col10\", \n",
    "                  \"col11\", \"col12\", \"col13\", \"col14\", \"col15\", \"col16\", \"col17\", \"col18\", \"col19\", \"col20\", \n",
    "                  \"col21\", \"col22\", \"col23\", \"col24\", \"col25\", \"col26\", \"col27\", \"col28\", \"col29\", \"col30\", \n",
    "                  \"col31\", \"col32\", \"col33\", \"col34\", \"col35\", \"col36\", \"col37\",\"col38\",\"col39\",\"col40\", \"col41\",\"label\")\n",
    "\n",
    "# Handle missing values\n",
    "imputer = Imputer(inputCols=[\"col5\", \"col6\", \"col7\", \"col8\", \"col9\", \"col10\", \"col11\", \"col12\", \"col13\",\n",
    "                             \"col14\", \"col15\", \"col16\", \"col17\", \"col18\", \"col19\", \"col20\", \"col21\", \"col22\",\n",
    "                             \"col23\", \"col24\", \"col25\", \"col26\", \"col27\", \"col28\", \"col29\", \"col30\", \"col31\",\n",
    "                             \"col32\", \"col33\", \"col34\", \"col35\", \"col36\", \"col37\",\"col38\",\"col39\",\"col40\", \"col41\"],\n",
    "                   outputCols=[\"col5\", \"col6\", \"col7\", \"col8\", \"col9\", \"col10\", \"col11\", \"col12\", \"col13\",\n",
    "                               \"col14\", \"col15\", \"col16\", \"col17\", \"col18\", \"col19\", \"col20\", \"col21\", \"col22\",\n",
    "                               \"col23\", \"col24\", \"col25\", \"col26\", \"col27\", \"col28\", \"col29\", \"col30\", \"col31\",\n",
    "                               \"col32\", \"col33\", \"col34\", \"col35\", \"col36\", \"col37\",\"col38\",\"col39\",\"col40\", \"col41\"]).setStrategy(\"mean\")\n",
    "\n",
    "data = imputer.fit(data).transform(data)\n",
    "\n",
    "# Scale numerical features\n",
    "feature_cols = [\"col5\", \"col6\", \"col7\", \"col8\", \"col9\", \"col10\", \"col11\", \"col12\", \"col13\", \n",
    "                \"col14\", \"col15\", \"col16\", \"col17\", \"col18\", \"col19\", \"col20\", \"col21\", \"col22\", \n",
    "                \"col23\", \"col24\", \"col25\", \"col26\", \"col27\", \"col28\", \"col29\", \"col30\", \"col31\", \n",
    "                \"col32\", \"col33\", \"col34\", \"col35\", \"col36\", \"col37\",\"col38\",\"col39\",\"col40\", \"col41\"]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "data = assembler.transform(data)\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "data = scaler.fit(data).transform(data)\n",
    "\n",
    "# K-means clustering\n",
    "kmeans = KMeans(k=80, seed=1, featuresCol=\"scaled_features\", predictionCol=\"cluster\")\n",
    "model = kmeans.fit(data)\n",
    "clusters = model.transform(data)\n",
    "\n",
    "# Get cluster centers\n",
    "centers = np.array(model.clusterCenters())\n",
    "\n",
    "# Calculate distances from points to cluster centers\n",
    "def distance_to_center(features, center):\n",
    "    return float(np.sqrt(np.sum((np.array(features) - np.array(center)) ** 2)))\n",
    "\n",
    "distance_udf = udf(lambda features: min([distance_to_center(features, center) for center in centers]), FloatType())\n",
    "\n",
    "clusters = clusters.withColumn(\"distance_to_center\", distance_udf(col(\"scaled_features\")))\n",
    "\n",
    "# Define anomaly if distance is greater than a threshold\n",
    "threshold = 1.0  # Set a suitable threshold\n",
    "clusters = clusters.withColumn(\"is_anomaly\", col(\"distance_to_center\") > threshold)\n",
    "\n",
    "# Evaluating K-Means Clustering for anomalies\n",
    "evaluator = ClusteringEvaluator(featuresCol=\"scaled_features\", predictionCol=\"cluster\")\n",
    "silhouette = evaluator.evaluate(clusters)\n",
    "print(f\"Silhouette with squared Euclidean distance = {silhouette}\")\n",
    "\n",
    "# Print the number of anomalies and normal points\n",
    "num_anomalies = clusters.filter(col(\"is_anomaly\")).count()\n",
    "num_normal = clusters.filter(~col(\"is_anomaly\")).count()\n",
    "total_count = clusters.count()\n",
    "\n",
    "print(f\"Number of anomalies: {num_anomalies}\")\n",
    "print(f\"Number of normal points: {num_normal}\")\n",
    "print(f\"Total number of points: {total_count}\")\n",
    "\n",
    "clusters.show(3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
